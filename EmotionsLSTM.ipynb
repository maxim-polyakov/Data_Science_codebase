{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Input\n",
    "from keras.models import Sequential\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle as p\n",
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer as token\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "EMBEDDING_VECTOR_LENGTH = 33"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T18:05:04.098628300Z",
     "start_time": "2023-05-19T18:05:04.078632600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    TOP_K = 20000\n",
    "    MAX_SEQUENCE_LENGTH = 33\n",
    "\n",
    "    def __init__(self, train_texts):\n",
    "        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ\n",
    "        #\n",
    "        self.train_texts = train_texts\n",
    "        self.tokenizer = token(num_words=self.TOP_K)\n",
    "\n",
    "    def train_tokenize(self):\n",
    "        #\n",
    "        #\n",
    "        max_length = len(max(self.train_texts, key=len))\n",
    "        self.max_length = min(max_length, self.MAX_SEQUENCE_LENGTH)\n",
    "        self.tokenizer.fit_on_texts(self.train_texts)\n",
    "\n",
    "    def vectorize_input(self, tweets):\n",
    "        #\n",
    "        #\n",
    "        tweets = self.tokenizer.texts_to_sequences(tweets)\n",
    "        tweets = pad_sequences(tweets, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        return tweets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T18:05:04.547480200Z",
     "start_time": "2023-05-19T18:05:04.528479500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "## –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑–µ –ª–µ–º–∞–Ω–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã\n",
    "class CommonPreprocessing:\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_text(cls, text):\n",
    "        # –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞ –≤ –Ω–∞—á–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É. –î–∞–Ω–Ω—ã–µ –≤ –±–∞–∑–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ª–µ–º–∞–Ω–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –≤ –ø—Ä–µ–¥–∏–∫—Ç —Å–ª–æ–≤–æ —Ç–æ–∂–µ –ª–µ–º–∞–Ω–∞—Ç–∏—Ä—É–µ—Ç—Å—è\n",
    "        #\n",
    "        try:\n",
    "            tokens = str(text)\n",
    "            tokens = Mystem().lemmatize(text.lower())\n",
    "            tokens = [token for token in tokens if token not in stopwords.words('russian')\n",
    "                      and token != ' '\n",
    "                      and token.strip() not in punctuation]\n",
    "            tokens = [\n",
    "                token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "            text = ' '.join(tokens).rstrip('\\n')\n",
    "            pattern3 = r'[\\d]'\n",
    "            pattern2 = '[.]'\n",
    "            text = re.sub(pattern3, '', text)\n",
    "            text = re.sub(pattern2, '', text)\n",
    "            text = re.sub('  ', ' ', text)\n",
    "            return text\n",
    "        except:\n",
    "            return 'The exception is in CommonPreprocessing.preprocess_text'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T18:05:05.098314700Z",
     "start_time": "2023-05-19T18:05:05.081317500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def predict(inpt, tmap, model, tokenizer):\n",
    "    #\n",
    "    #\n",
    "    model = load_model(model)\n",
    "    inn = []\n",
    "    pr = CommonPreprocessing()\n",
    "    for i in inpt:\n",
    "        inn.append(pr.preprocess_text(i))\n",
    "\n",
    "    with open(tokenizer, 'rb') as handle:\n",
    "        tokenizer = p.load(handle)\n",
    "\n",
    "    tokenized_inpt = tokenizer.vectorize_input(inn)\n",
    "    scoreplu = model.predict(tokenized_inpt)\n",
    "    outpt = tmap[scoreplu.argmax(axis=-1)[0]]\n",
    "    return outpt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T18:05:05.686052800Z",
     "start_time": "2023-05-19T18:05:05.667734200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "'üòä'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMOTIONSMAPA = {0: 'üòû', 1: 'ü§¨', 2: 'üò®', 3: 'üòä', 4: '‚ù§', 5: 'üò≥', 6: ''}\n",
    "modelpath = next(Path().rglob('0_lstmemotionsmodel.h5'))\n",
    "tokenizerpath = next(Path().rglob('0_lstmemotionstokenizer.pickle'))\n",
    "emotion = predict(\"—ç—Ç–æ —Å–µ–º–ø–ª –∏–∑ —Ç—Ä–µ–π–Ω–∞ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–∏–∫—Ç –Ω–∞ —Ç–µ–∫—Å—Ç–µ —á—Ç–æ –Ω–µ –≤ —Ç—Ä–µ–π–Ω–µ –¥–µ–ª–∞—Ç—å –∏ –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ —Ç–∫ —Ç–∞–º –≤—Å—ë –æ–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ\",EMOTIONSMAPA, 'Models/0_lstmemotionsmodel.h5', 'Tokenizers/0_lstmemotionstokenizer.pickle')\n",
    "emotion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T18:10:37.509023300Z",
     "start_time": "2023-05-19T18:08:52.692024Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
